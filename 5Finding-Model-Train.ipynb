{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6d4e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[1]: Import Required Libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# In[2]: Load and Split Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d6d363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Class Distribution:\n",
      "Finding Labels\n",
      "No Finding      5128\n",
      "Infiltration    3004\n",
      "Nodule          2886\n",
      "Pneumothorax    2838\n",
      "Atelectasis     2397\n",
      "Effusion        2320\n",
      "Name: count, dtype: int64\n",
      "Train size: 13001, Validation size: 2786, Test size: 2786\n"
     ]
    }
   ],
   "source": [
    "# In[2]: Load Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths /Users/zafercalisir/Desktop/GradProj/CheXnet-App/ChestX-ray14/Combined_Train_and_Test_Data.csv\n",
    "data_csv = \"ChestX-ray14/Filtered_Cases_with_One_or_No_Finding.csv\"\n",
    "image_directory = \"/Users/zafercalisir/Desktop/GradProj/cnn-in-nih14/dataset/images-224/images-224\"\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(data_csv)\n",
    "\n",
    "# Filter out labels with fewer than 2 samples\n",
    "filtered_data = data.groupby('Finding Labels').filter(lambda x: len(x) > 5)\n",
    "\n",
    "# Verify filtered class distribution\n",
    "print(\"Filtered Class Distribution:\")\n",
    "print(filtered_data['Finding Labels'].value_counts())\n",
    "\n",
    "# Split the filtered dataset into train, validation, and test sets\n",
    "# Step 1: Train/Test Split\n",
    "train_data, temp_data = train_test_split(\n",
    "    filtered_data, \n",
    "    test_size=0.3, \n",
    "    stratify=filtered_data['Finding Labels'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Validation/Test Split\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data, \n",
    "    test_size=0.5, \n",
    "    stratify=temp_data['Finding Labels'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Display dataset sizes\n",
    "print(f\"Train size: {len(train_data)}, Validation size: {len(val_data)}, Test size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da744d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXRayDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Validate the dataset\n",
    "        valid_samples = []\n",
    "        for idx in range(len(self.dataframe)):\n",
    "            row = self.dataframe.iloc[idx]\n",
    "            image_path = os.path.join(self.image_dir, row['Image Index'])\n",
    "            if os.path.exists(image_path):\n",
    "                valid_samples.append(row)\n",
    "            else:\n",
    "                print(f\"Missing image: {image_path}\")\n",
    "        self.dataframe = pd.DataFrame(valid_samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, row['Image Index'])\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"L\")  # Convert to grayscale\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self.dataframe))  # Skip problematic image\n",
    "\n",
    "        label = row['Finding Labels']\n",
    "        findings = [\"Infiltration\", \"Atelectasis\", \"Effusion\", \"Nodule\", \"Pneumothorax\", \"No Finding\"]\n",
    "        label_vector = [1 if finding in label else 0 for finding in findings]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label_vector, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b668813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class ChestXRayDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_directory, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_directory = image_directory\n",
    "        self.transform = transform\n",
    "        # Define the label mapping\n",
    "        self.label_mapping = {\n",
    "            'No Finding': [1, 0, 0, 0, 0, 0],\n",
    "            'Pneumonia': [0, 1, 0, 0, 0, 0],\n",
    "            'Infiltration': [0, 0, 1, 0, 0, 0],\n",
    "            'Tuberculosis': [0, 0, 0, 1, 0, 0],\n",
    "            'Cardiomegaly': [0, 0, 0, 0, 1, 0],\n",
    "            'Pneumothorax': [0, 0, 0, 0, 0, 1],\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_path = f\"{self.image_directory}/{row['Image Index']}\"  # Use correct column name\n",
    "        image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "        # Map categorical label to numeric encoding\n",
    "        label_str = row['Finding Labels']  # Assuming label is a string or comma-separated list\n",
    "        if ',' in label_str:  # Handle multi-label cases\n",
    "            label = [0] * len(self.label_mapping)  # Initialize a zero vector\n",
    "            for label_part in label_str.split(','):\n",
    "                label_part = label_part.strip()\n",
    "                if label_part in self.label_mapping:\n",
    "                    label = [x + y for x, y in zip(label, self.label_mapping[label_part])]\n",
    "        else:\n",
    "            label = self.label_mapping.get(label_str.strip(), [0, 0, 0, 0, 0, 0])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Data Augmentation and Normalization\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = ChestXRayDataset(train_data, image_directory, transform=train_transform)\n",
    "val_dataset = ChestXRayDataset(val_data, image_directory, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "529e3951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zafercalisir/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/zafercalisir/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load Pretrained ResNet50\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Modify the First Layer for Grayscale Input\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Modify the Fully Connected Layer for Multi-Label Classification\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(128, 6)  # Output for 6 classes\n",
    ")\n",
    "\n",
    "# Initialize Weights for New Layers\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "model.fc.apply(init_weights)\n",
    "\n",
    "# Loss, Optimizer, and Scheduler\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# Fine-tuning the Last Few Layers\n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Freeze Earlier Layers\n",
    "for param in list(model.parameters())[:-len(list(model.layer4.parameters()))]:\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Move Model to Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bc939a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13001, 14)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)  # Should print (num_samples, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58ac1998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: tensor([2103., 1678., 1624., 2020., 1987., 3589.])\n"
     ]
    }
   ],
   "source": [
    "# Convert class counts to tensor\n",
    "class_distribution = torch.tensor([class_counts[finding] for finding in findings], dtype=torch.float)\n",
    "print(f\"Class distribution: {class_distribution}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "baf1f607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  16%|█▋        | 67/407 [05:02<25:33,  4.51s/it, Training Loss=0.0589]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 56\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     58\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torchvision/models/resnet.py:274\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[1;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m--> 274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[1;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torchvision/models/resnet.py:157\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    154\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(out)\n\u001b[1;32m    155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[1;32m    160\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Define EarlyStopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, path='best_model.pth.tar'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "\n",
    "    def __call__(self, val_loss, model, optimizer, epoch):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, optimizer, epoch)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, optimizer, epoch)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, optimizer, epoch):\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# Initialize EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True, path='best_model.pth.tar')\n",
    "\n",
    "# Training Loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=True)\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({\"Training Loss\": running_loss / len(progress_bar)})\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping\n",
    "    early_stopping(val_loss, model, optimizer, epoch)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Load the Best Model\n",
    "checkpoint = torch.load('best_model.pth.tar')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']} with validation loss: {checkpoint['val_loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22c5d4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y2/m1n7lqf10t99jk_mtznp43yh0000gn/T/ipykernel_14670/2210013568.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('best_model.pth.tar')\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('best_model.pth.tar')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a557455d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAHHCAYAAACvJxw8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/7klEQVR4nO3deVhV5f7//9dmngQUlUFR1HAik9IwtNKSE5qmpqYSOR3LPM5pftTjWOd0rMyTlaXZVZqnzKm0OUO0MiXHnMXKHDAFUgOcQIT790c/9rctuARjEHs+rmtdyr3utdb7Xmzbr9a699o2Y4wRAAAAiuRU0QUAAABczwhLAAAAFghLAAAAFghLAAAAFghLAAAAFghLAAAAFghLAAAAFghLAAAAFghLAAAAFghLQCVw+PBh2Ww2LVy40N42ffp02Wy2Ym1vs9k0ffr0Uq2pXbt2ateuXanuE7BS8O/ghRdeqOhS8BdDWAJKWZcuXeTl5aUzZ85csU98fLzc3Nx06tSpcqys5Pbt26fp06fr8OHDFV2K3VdffSWbzaYVK1ZUdCnFsnfvXj3yyCOqVauW3N3dFRISovj4eO3du7eiSyukIIxcaXn22WcrukSgQrhUdAHAjSY+Pl4ff/yxVq5cqX79+hVaf/78eX344Yfq0KGDAgICrvk4kydP1oQJE/5MqVe1b98+PfXUU2rXrp3CwsIc1n355ZdleuwbwQcffKC4uDhVq1ZNgwYNUr169XT48GG9+eabWrFihZYsWaIHH3ywosssJC4uTvfff3+h9ltvvbUCqgEqHmEJKGVdunRRlSpVtHjx4iLD0ocffqhz584pPj7+Tx3HxcVFLi4V90/Yzc2two5dGRw8eFB9+/ZV/fr19c0336hGjRr2daNGjdJdd92lvn37ateuXapfv3651XXu3Dl5e3tb9rntttv0yCOPlFNFwPWP23BAKfP09FT37t2VmJio9PT0QusXL16sKlWqqEuXLjp9+rSefPJJNWvWTD4+PvL19VXHjh21c+fOqx6nqDlLOTk5euKJJ1SjRg37MY4dO1Zo2yNHjmjo0KFq1KiRPD09FRAQoIceesjhdtvChQv10EMPSZLuuece+62Yr776SlLRc5bS09M1aNAgBQYGysPDQ82bN9fbb7/t0OeP807mz5+vBg0ayN3dXbfffru2bNly1XEX188//6yHHnpI1apVk5eXl+644w59+umnhfq98sorioiIkJeXl6pWraqWLVtq8eLF9vVnzpzR6NGjFRYWJnd3d9WsWVN/+9vftH37dsvjz5w5U+fPn9f8+fMdgpIkVa9eXa+//rrOnTun559/XpK0YsUK2Ww2ff3114X29frrr8tms2nPnj32tuTkZPXs2VPVqlWTh4eHWrZsqY8++shhu4ULF9r3OXToUNWsWVO1a9e++skrhrCwMHXu3FlffvmlIiMj5eHhoaZNm+qDDz4o1Le4v4vs7GxNnz5dDRs2lIeHh4KDg9W9e3cdPHiwUN+rvXZSU1M1cOBA1a5dW+7u7goODlbXrl2vq1vKqDy4sgSUgfj4eL399ttatmyZhg8fbm8/ffq0Vq9erbi4OHl6emrv3r1atWqVHnroIdWrV09paWl6/fXX1bZtW+3bt08hISElOu6jjz6qd955Rw8//LBat26ttWvXqlOnToX6bdmyRRs3blSfPn1Uu3ZtHT58WHPnzlW7du20b98+eXl56e6779bIkSP18ssv65///KeaNGkiSfY/L3fhwgW1a9dOP/30k4YPH6569epp+fLlGjBggDIyMjRq1CiH/osXL9aZM2f0+OOPy2az6fnnn1f37t31888/y9XVtUTjvlxaWppat26t8+fPa+TIkQoICNDbb7+tLl26aMWKFfZbX2+88YZGjhypnj17atSoUcrOztauXbu0adMmPfzww5KkIUOGaMWKFRo+fLiaNm2qU6dO6dtvv9X+/ft12223XbGGjz/+WGFhYbrrrruKXH/33XcrLCzMHho6deokHx8fLVu2TG3btnXou3TpUkVEROjmm2+W9Ps8qDZt2qhWrVqaMGGCvL29tWzZMnXr1k3vv/9+oVt7Q4cOVY0aNTR16lSdO3fuqufv/PnzOnnyZKF2f39/h6uZP/74o3r37q0hQ4aof//+WrBggR566CF98cUX+tvf/iap+L+LvLw8de7cWYmJierTp49GjRqlM2fOKCEhQXv27FGDBg3sxy3Oa6dHjx7au3evRowYobCwMKWnpyshIUFHjx4tdEsZuCoDoNRdunTJBAcHm+joaIf2efPmGUlm9erVxhhjsrOzTV5enkOfQ4cOGXd3d/P00087tEkyCxYssLdNmzbN/PGf8I4dO4wkM3ToUIf9Pfzww0aSmTZtmr3t/PnzhWpOSkoyksyiRYvsbcuXLzeSzLp16wr1b9u2rWnbtq3959mzZxtJ5p133rG3Xbx40URHRxsfHx+TlZXlMJaAgABz+vRpe98PP/zQSDIff/xxoWP90bp164wks3z58iv2GT16tJFk1q9fb287c+aMqVevngkLC7Of865du5qIiAjL4/n5+Zlhw4ZZ9rlcRkaGkWS6du1q2a9Lly5Gkv3cxMXFmZo1a5pLly7Z+5w4ccI4OTk5vB7at29vmjVrZrKzs+1t+fn5pnXr1iY8PNzetmDBAiPJ3HnnnQ77vJKC382VlqSkJHvfunXrGknm/ffft7dlZmaa4OBgc+utt9rbivu7eOutt4wk89///rdQXfn5+Q71Xe2189tvvxlJZubMmVcdM1Ac3IYDyoCzs7P69OmjpKQkh8v+ixcvVmBgoNq3by9Jcnd3l5PT7/8M8/LydOrUKfn4+KhRo0ZXvc1zuc8++0ySNHLkSIf20aNHF+rr6elp/3tubq5OnTqlm266Sf7+/iU+7h+PHxQUpLi4OHubq6urRo4cqbNnzxa6vdS7d29VrVrV/nPBFZiff/75mo5/eS1RUVG688477W0+Pj4aPHiwDh8+rH379kn6/UrJsWPHLG//+fv7a9OmTTp+/Hixj1/wScgqVapY9itYn5WVJen3c5Kenm6/1Sn9fnsuPz9fvXv3lvT71cm1a9eqV69eOnPmjE6ePKmTJ0/q1KlTio2N1Y8//qhffvnF4TiPPfaYnJ2di13/4MGDlZCQUGhp2rSpQ7+QkBCHq1i+vr7q16+fvv/+e6Wmpkoq/u/i/fffV/Xq1TVixIhC9Vx+u/lqrx1PT0+5ubnpq6++0m+//VbscQNXQlgCykjBBO6C+S/Hjh3T+vXr1adPH/sbV35+vl588UWFh4fL3d1d1atXV40aNbRr1y5lZmaW6HhHjhyRk5OTw+0KSWrUqFGhvhcuXNDUqVMVGhrqcNyMjIwSH/ePxw8PD7eHvwIFt+2OHDni0F6nTh2Hnwve/Erjze3IkSNFjvvyWsaPHy8fHx9FRUUpPDxcw4YN04YNGxy2ef7557Vnzx6FhoYqKipK06dPv2qgKwhBVo+P+OP6gv4dOnSQn5+fli5dau+zdOlSRUZGqmHDhpKkn376ScYYTZkyRTVq1HBYpk2bJkmF5srVq1fPso7LhYeHKyYmptDi6+vr0O+mm24qFGQK6iz4n4Ti/i4OHjyoRo0aFetDC1d77bi7u+u5557T559/rsDAQN199916/vnn7QEOKCnCElBGWrRoocaNG+u9996TJL333nsyxjh8Cu4///mPxowZo7vvvlvvvPOOVq9erYSEBEVERCg/P7/MahsxYoSeeeYZ9erVS8uWLdOXX36phIQEBQQElOlx/+hKVzqMMeVyfOn3N+wDBw5oyZIluvPOO/X+++/rzjvvtIcOSerVq5d+/vlnvfLKKwoJCdHMmTMVERGhzz///Ir79fPzU3BwsHbt2mV5/F27dqlWrVr2EOLu7q5u3bpp5cqVunTpkn755Rdt2LDBflVJkv338+STTxZ59SchIUE33XSTw3H+eCXxRlCc187o0aP1ww8/aMaMGfLw8NCUKVPUpEkTff/99+VVJm4gTPAGylB8fLymTJmiXbt2afHixQoPD9ftt99uX79ixQrdc889evPNNx22y8jIUPXq1Ut0rLp16yo/P9/+f+gFDhw4UKjvihUr1L9/f82aNcvelp2drYyMDId+xX1CeMHxd+3apfz8fIerS8nJyfb15aVu3bpFjruoWry9vdW7d2/17t1bFy9eVPfu3fXMM89o4sSJ8vDwkCQFBwdr6NChGjp0qNLT03XbbbfpmWeeUceOHa9YQ+fOnfXGG2/o22+/dbgFVWD9+vU6fPiwHn/8cYf23r176+2331ZiYqL2798vY4xDWCp4zICrq6tiYmJKcFZKX8FVrj++Tn744QdJsk+iLu7vokGDBtq0aZNyc3P/9AT/Ag0aNNDYsWM1duxY/fjjj4qMjNSsWbP0zjvvlMr+8dfBlSWgDBVcRZo6dap27NhR6NlKzs7Oha6kLF++vNCck+IoeON++eWXHdpnz55dqG9Rx33llVeUl5fn0FbwPJ7LQ1RR7r//fqWmpjrcQrp06ZJeeeUV+fj4FPqEV1m6//77tXnzZiUlJdnbzp07p/nz5yssLMw+9+byJ6i7ubmpadOmMsYoNzdXeXl5hW5L1qxZUyEhIcrJybGsYdy4cfL09NTjjz9e6DinT5/WkCFD5OXlpXHjxjmsi4mJUbVq1bR06VItXbpUUVFRDrfRatasqXbt2un111/XiRMnCh33119/tayrNB0/flwrV660/5yVlaVFixYpMjJSQUFBkor/u+jRo4dOnjypOXPmFDpOSa82nj9/XtnZ2Q5tDRo0UJUqVa76ewOKwpUloAzVq1dPrVu31ocffihJhcJS586d9fTTT2vgwIFq3bq1du/erXffffeaHlIYGRmpuLg4vfbaa8rMzFTr1q2VmJion376qVDfzp0763//+5/8/PzUtGlTJSUlac2aNYWeKB4ZGSlnZ2c999xzyszMlLu7u+69917VrFmz0D4HDx6s119/XQMGDNC2bdsUFhamFStWaMOGDZo9e/ZVJzuX1Pvvv2+/OvFH/fv314QJE/Tee++pY8eOGjlypKpVq6a3335bhw4d0vvvv2+/8nXfffcpKChIbdq0UWBgoPbv3685c+aoU6dOqlKlijIyMlS7dm317NlTzZs3l4+Pj9asWaMtW7Y4XJUrSnh4uN5++23Fx8erWbNmhZ7gffLkSb333nuF5pi5urqqe/fuWrJkic6dO1fk96C9+uqruvPOO9WsWTM99thjql+/vtLS0pSUlKRjx44V6zldVrZv317k1ZcGDRooOjra/nPDhg01aNAgbdmyRYGBgXrrrbeUlpamBQsW2PsU93fRr18/LVq0SGPGjNHmzZt111136dy5c1qzZo2GDh2qrl27Frv+H374Qe3bt1evXr3UtGlTubi4aOXKlUpLS1OfPn3+xJnBX1ZFfQwP+Kt49dVXjSQTFRVVaF12drYZO3asCQ4ONp6enqZNmzYmKSmp0Mfyi/PoAGOMuXDhghk5cqQJCAgw3t7e5oEHHjApKSmFHh3w22+/mYEDB5rq1asbHx8fExsba5KTk03dunVN//79Hfb5xhtvmPr16xtnZ2eHxwhcXqMxxqSlpdn36+bmZpo1a+ZQ8x/HUtTHui+vsygFjw640lLwEfWDBw+anj17Gn9/f+Ph4WGioqLMJ5984rCv119/3dx9990mICDAuLu7mwYNGphx48aZzMxMY4wxOTk5Zty4caZ58+amSpUqxtvb2zRv3ty89tprljX+0a5du0xcXJwJDg42rq6uJigoyMTFxZndu3dfcZuEhAQjydhsNpOSklJkn4MHD5p+/fqZoKAg4+rqamrVqmU6d+5sVqxYYe9T8OiALVu2FKvWqz064I+vjbp165pOnTqZ1atXm1tuucW4u7ubxo0bF/lIh+L8Loz5/ZEWkyZNMvXq1bOfq549e5qDBw861He1187JkyfNsGHDTOPGjY23t7fx8/MzrVq1MsuWLSvWeQAuZzOmHGdTAgBuCGFhYbr55pv1ySefVHQpQJljzhIAAIAFwhIAAIAFwhIAAIAF5iwBAABY4MoSAACABcISAACABR5KWQry8/N1/PhxValSpURfDwEAACqOMUZnzpxRSEhIoS8B/yPCUik4fvy4QkNDK7oMAABwDVJSUlS7du0rricslYKCr3FISUmxf3s4AAC4vmVlZSk0NPSqX8dEWCoFBbfefH19CUsAAFQyV5tCwwRvAAAAC4QlAAAAC4QlAAAAC8xZAgBUuLy8POXm5lZ0GbjBuLq6ytnZ+U/vh7AEAKgwxhilpqYqIyOjokvBDcrf319BQUF/6jmIhCUAQIUpCEo1a9aUl5cXD/ZFqTHG6Pz580pPT5ckBQcHX/O+CEsAgAqRl5dnD0oBAQEVXQ5uQJ6enpKk9PR01axZ85pvyTHBGwBQIQrmKHl5eVVwJbiRFby+/sycOMISAKBCcesNZak0Xl+EJQAAAAuEJQAAKkC7du00evRo+89hYWGaPXu25TY2m02rVq3608curf38VRCWAAAogQceeEAdOnQoct369etls9m0a9euEu93y5YtGjx48J8tz8H06dMVGRlZqP3EiRPq2LFjqR7rcgsXLpS/v3+ZHqO8EJYAACiBQYMGKSEhQceOHSu0bsGCBWrZsqVuueWWEu+3Ro0a5TbZPSgoSO7u7uVyrBsBYQkAgBLo3LmzatSooYULFzq0nz17VsuXL9egQYN06tQpxcXFqVatWvLy8lKzZs303nvvWe738ttwP/74o+6++255eHioadOmSkhIKLTN+PHj1bBhQ3l5eal+/fqaMmWK/VNfCxcu1FNPPaWdO3fKZrPJZrPZa778Ntzu3bt17733ytPTUwEBARo8eLDOnj1rXz9gwAB169ZNL7zwgoKDgxUQEKBhw4b9qU+YHT16VF27dpWPj498fX3Vq1cvpaWl2dfv3LlT99xzj6pUqSJfX1+1aNFCW7dulSQdOXJEDzzwgKpWrSpvb29FRETos88+u+ZarobnLAEArhvGGF3IzSv343q6Ohf7U1MuLi7q16+fFi5cqEmTJtm3W758ufLy8hQXF6ezZ8+qRYsWGj9+vHx9ffXpp5+qb9++atCggaKioq56jPz8fHXv3l2BgYHatGmTMjMzHeY3FahSpYoWLlyokJAQ7d69W4899piqVKmi//u//1Pv3r21Z88effHFF1qzZo0kyc/Pr9A+zp07p9jYWEVHR2vLli1KT0/Xo48+quHDhzsEwnXr1ik4OFjr1q3TTz/9pN69eysyMlKPPfZYsc7b5eMrCEpff/21Ll26pGHDhql379766quvJEnx8fG69dZbNXfuXDk7O2vHjh1ydXWVJA0bNkwXL17UN998I29vb+3bt08+Pj4lrqO4CEsAgOvGhdw8NZ26utyPu+/pWHm5Ff8t8e9//7tmzpypr7/+Wu3atZP0+y24Hj16yM/PT35+fnryySft/UeMGKHVq1dr2bJlxQpLa9asUXJyslavXq2QkBBJ0n/+859C84wmT55s/3tYWJiefPJJLVmyRP/3f/8nT09P+fj4yMXFRUFBQVc81uLFi5Wdna1FixbJ29tbkjRnzhw98MADeu655xQYGChJqlq1qubMmSNnZ2c1btxYnTp1UmJi4jWFpcTERO3evVuHDh1SaGioJGnRokWKiIjQli1bdPvtt+vo0aMaN26cGjduLEkKDw+3b3/06FH16NFDzZo1kyTVr1+/xDWUBLfhAAAoocaNG6t169Z66623JEk//fST1q9fr0GDBkn6/enk//rXv9SsWTNVq1ZNPj4+Wr16tY4ePVqs/e/fv1+hoaH2oCRJ0dHRhfotXbpUbdq0UVBQkHx8fDR58uRiH+OPx2revLk9KElSmzZtlJ+frwMHDtjbIiIiHJ6AHRwcbP8qkZIqGF9BUJKkpk2byt/fX/v375ckjRkzRo8++qhiYmL07LPP6uDBg/a+I0eO1L///W+1adNG06ZNu6YJ9SXBlSUAwHXD09VZ+56OrZDjltSgQYM0YsQIvfrqq1qwYIEaNGigtm3bSpJmzpypl156SbNnz1azZs3k7e2t0aNH6+LFi6VWc1JSkuLj4/XUU08pNjZWfn5+WrJkiWbNmlVqx/ijgltgBWw2m/Lz88vkWNLvn+R7+OGH9emnn+rzzz/XtGnTtGTJEj344IN69NFHFRsbq08//VRffvmlZsyYoVmzZmnEiBFlUgtXlgAA1w2bzSYvN5dyX67lKc+9evWSk5OTFi9erEWLFunvf/+7fT8bNmxQ165d9cgjj6h58+aqX7++fvjhh2Lvu0mTJkpJSdGJEyfsbd99951Dn40bN6pu3bqaNGmSWrZsqfDwcB05csShj5ubm/LyrOeANWnSRDt37tS5c+fsbRs2bJCTk5MaNWpU7JpLomB8KSkp9rZ9+/YpIyNDTZs2tbc1bNhQTzzxhL788kt1795dCxYssK8LDQ3VkCFD9MEHH2js2LF64403yqRWibAEAMA18fHxUe/evTVx4kSdOHFCAwYMsK8LDw9XQkKCNm7cqP379+vxxx93+KTX1cTExKhhw4bq37+/du7cqfXr12vSpEkOfcLDw3X06FEtWbJEBw8e1Msvv6yVK1c69AkLC9OhQ4e0Y8cOnTx5Ujk5OYWOFR8fLw8PD/Xv31979uzRunXrNGLECPXt29c+X+la5eXlaceOHQ7L/v37FRMTo2bNmik+Pl7bt2/X5s2b1a9fP7Vt21YtW7bUhQsXNHz4cH311Vc6cuSINmzYoC1btqhJkyaSpNGjR2v16tU6dOiQtm/frnXr1tnXlQXCEgAA12jQoEH67bffFBsb6zC/aPLkybrtttsUGxurdu3aKSgoSN26dSv2fp2cnLRy5UpduHBBUVFRevTRR/XMM8849OnSpYueeOIJDR8+XJGRkdq4caOmTJni0KdHjx7q0KGD7rnnHtWoUaPIxxd4eXlp9erVOn36tG6//Xb17NlT7du315w5c0p2Mopw9uxZ3XrrrQ7LAw88IJvNpg8//FBVq1bV3XffrZiYGNWvX19Lly6VJDk7O+vUqVPq16+fGjZsqF69eqljx4566qmnJP0ewoYNG6YmTZqoQ4cOatiwoV577bU/Xe+V2Iwxpsz2/heRlZUlPz8/ZWZmytfXt6LLAYBKITs7W4cOHVK9evXk4eFR0eXgBmX1Oivu+zdXlgAAACwQlgAAACwQlgAAACwQlgAAACwQlgAAFYrPGaEslcbri7AEAKgQBU+EPn/+fAVXghtZwevr8ieQlwRfdwIAqBDOzs7y9/e3f7+Yl5fXNT1JGyiKMUbnz59Xenq6/P39Hb7XrqQISwCAChMUFCRJ1/yFrMDV+Pv7219n14qwBACoMDabTcHBwapZs6Zyc3MruhzcYFxdXf/UFaUChCUAQIVzdnYulTc1oCwwwRsAAMACYQkAAMACYQkAAMACYQkAAMACYQkAAMACYQkAAMACYQkAAMACYQkAAMACYQkAAMACYQkAAMACYQkAAMBCpQtLr776qsLCwuTh4aFWrVpp8+bNlv2XL1+uxo0by8PDQ82aNdNnn312xb5DhgyRzWbT7NmzS7lqAABQWVWqsLR06VKNGTNG06ZN0/bt29W8eXPFxsYqPT29yP4bN25UXFycBg0apO+//17dunVTt27dtGfPnkJ9V65cqe+++04hISFlPQwAAFCJVKqw9N///lePPfaYBg4cqKZNm2revHny8vLSW2+9VWT/l156SR06dNC4cePUpEkT/etf/9Jtt92mOXPmOPT75ZdfNGLECL377rtydXUtj6EAAIBKotKEpYsXL2rbtm2KiYmxtzk5OSkmJkZJSUlFbpOUlOTQX5JiY2Md+ufn56tv374aN26cIiIiyqZ4AABQablUdAHFdfLkSeXl5SkwMNChPTAwUMnJyUVuk5qaWmT/1NRU+8/PPfecXFxcNHLkyGLXkpOTo5ycHPvPWVlZxd4WAABULpXmylJZ2LZtm1566SUtXLhQNput2NvNmDFDfn5+9iU0NLQMqwQAABWp0oSl6tWry9nZWWlpaQ7taWlpCgoKKnKboKAgy/7r169Xenq66tSpIxcXF7m4uOjIkSMaO3aswsLCrljLxIkTlZmZaV9SUlL+3OAAAMB1q9KEJTc3N7Vo0UKJiYn2tvz8fCUmJio6OrrIbaKjox36S1JCQoK9f9++fbVr1y7t2LHDvoSEhGjcuHFavXr1FWtxd3eXr6+vwwIAAG5MlWbOkiSNGTNG/fv3V8uWLRUVFaXZs2fr3LlzGjhwoCSpX79+qlWrlmbMmCFJGjVqlNq2batZs2apU6dOWrJkibZu3ar58+dLkgICAhQQEOBwDFdXVwUFBalRo0blOzgAAHBdqlRhqXfv3vr11181depUpaamKjIyUl988YV9EvfRo0fl5PT/Lpa1bt1aixcv1uTJk/XPf/5T4eHhWrVqlW6++eaKGgIAAKhkbMYYU9FFVHZZWVny8/NTZmYmt+QAAKgkivv+XWnmLAEAAFQEwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAIAFwhIAAICFSheWXn31VYWFhcnDw0OtWrXS5s2bLfsvX75cjRs3loeHh5o1a6bPPvvMvi43N1fjx49Xs2bN5O3trZCQEPXr10/Hjx8v62EAAIBKolKFpaVLl2rMmDGaNm2atm/frubNmys2Nlbp6elF9t+4caPi4uI0aNAgff/99+rWrZu6deumPXv2SJLOnz+v7du3a8qUKdq+fbs++OADHThwQF26dCnPYQEAgOuYzRhjKrqI4mrVqpVuv/12zZkzR5KUn5+v0NBQjRgxQhMmTCjUv3fv3jp37pw++eQTe9sdd9yhyMhIzZs3r8hjbNmyRVFRUTpy5Ijq1KlTrLqysrLk5+enzMxM+fr6XsPIAABAeSvu+3elubJ08eJFbdu2TTExMfY2JycnxcTEKCkpqchtkpKSHPpLUmxs7BX7S1JmZqZsNpv8/f1LpW4AAFC5uVR0AcV18uRJ5eXlKTAw0KE9MDBQycnJRW6TmppaZP/U1NQi+2dnZ2v8+PGKi4uzTJg5OTnKycmx/5yVlVXcYQAAgEqm0lxZKmu5ubnq1auXjDGaO3euZd8ZM2bIz8/PvoSGhpZTlQAAoLxVmrBUvXp1OTs7Ky0tzaE9LS1NQUFBRW4TFBRUrP4FQenIkSNKSEi46ryjiRMnKjMz076kpKRcw4gAAEBlUGnCkpubm1q0aKHExER7W35+vhITExUdHV3kNtHR0Q79JSkhIcGhf0FQ+vHHH7VmzRoFBARctRZ3d3f5+vo6LAAA4MZUaeYsSdKYMWPUv39/tWzZUlFRUZo9e7bOnTungQMHSpL69eunWrVqacaMGZKkUaNGqW3btpo1a5Y6deqkJUuWaOvWrZo/f76k34NSz549tX37dn3yySfKy8uzz2eqVq2a3NzcKmagAADgulGpwlLv3r3166+/aurUqUpNTVVkZKS++OIL+yTuo0ePysnp/10sa926tRYvXqzJkyfrn//8p8LDw7Vq1SrdfPPNkqRffvlFH330kSQpMjLS4Vjr1q1Tu3btymVcAADg+lWpnrN0veI5SwAAVD433HOWAAAAKgJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwMI1haWUlBQdO3bM/vPmzZs1evRozZ8/v9QKAwAAuB5cU1h6+OGHtW7dOklSamqq/va3v2nz5s2aNGmSnn766VItEAAAoCJdU1jas2ePoqKiJEnLli3TzTffrI0bN+rdd9/VwoULS7M+AACACnVNYSk3N1fu7u6SpDVr1qhLly6SpMaNG+vEiROlVx0AAEAFu6awFBERoXnz5mn9+vVKSEhQhw4dJEnHjx9XQEBAqRYIAABQka4pLD333HN6/fXX1a5dO8XFxal58+aSpI8++sh+ew4AAOBGYDPGmGvZMC8vT1lZWapataq97fDhw/Ly8lLNmjVLrcDKICsrS35+fsrMzJSvr29FlwMAAIqhuO/f13Rl6cKFC8rJybEHpSNHjmj27Nk6cODAXy4oAQCAG9s1haWuXbtq0aJFkqSMjAy1atVKs2bNUrdu3TR37txSLfByr776qsLCwuTh4aFWrVpp8+bNlv2XL1+uxo0by8PDQ82aNdNnn33msN4Yo6lTpyo4OFienp6KiYnRjz/+WJZDAAAAlcg1haXt27frrrvukiStWLFCgYGBOnLkiBYtWqSXX365VAv8o6VLl2rMmDGaNm2atm/frubNmys2Nlbp6elF9t+4caPi4uI0aNAgff/99+rWrZu6deumPXv22Ps8//zzevnllzVv3jxt2rRJ3t7eio2NVXZ2dpmNAwAAVB7XNGfJy8tLycnJqlOnjnr16qWIiAhNmzZNKSkpatSokc6fP18WtapVq1a6/fbbNWfOHElSfn6+QkNDNWLECE2YMKFQ/969e+vcuXP65JNP7G133HGHIiMjNW/ePBljFBISorFjx+rJJ5+UJGVmZiowMFALFy5Unz59ilUXc5YAAKh8ynTO0k033aRVq1YpJSVFq1ev1n333SdJSk9PL7OwcPHiRW3btk0xMTH2NicnJ8XExCgpKanIbZKSkhz6S1JsbKy9/6FDh5SamurQx8/PT61atbriPiUpJydHWVlZDgsAALgxXVNYmjp1qp588kmFhYUpKipK0dHRkqQvv/xSt956a6kWWODkyZPKy8tTYGCgQ3tgYKBSU1OL3CY1NdWyf8GfJdmnJM2YMUN+fn72JTQ0tMTjAQAAlcM1haWePXvq6NGj2rp1q1avXm1vb9++vV588cVSK+56NXHiRGVmZtqXlJSUii4JAACUEZdr3TAoKEhBQUE6duyYJKl27dpl+kDK6tWry9nZWWlpaQ7taWlpCgoKumKNVv0L/kxLS1NwcLBDn8jIyCvW4u7ubv+6FwAAcGO7pitL+fn5evrpp+Xn56e6deuqbt268vf317/+9S/l5+eXdo2SJDc3N7Vo0UKJiYkOdSQmJtpvA14uOjraob8kJSQk2PvXq1dPQUFBDn2ysrK0adOmK+4TAAD8tVzTlaVJkybpzTff1LPPPqs2bdpIkr799ltNnz5d2dnZeuaZZ0q1yAJjxoxR//791bJlS0VFRWn27Nk6d+6cBg4cKEnq16+fatWqpRkzZkiSRo0apbZt22rWrFnq1KmTlixZoq1bt2r+/PmSJJvNptGjR+vf//63wsPDVa9ePU2ZMkUhISHq1q1bmYwBAABUMuYaBAcHmw8//LBQ+6pVq0xISMi17LLYXnnlFVOnTh3j5uZmoqKizHfffWdf17ZtW9O/f3+H/suWLTMNGzY0bm5uJiIiwnz66acO6/Pz882UKVNMYGCgcXd3N+3btzcHDhwoUU2ZmZlGksnMzLzmcQEAgPJV3Pfva3rOkoeHh3bt2qWGDRs6tB84cECRkZG6cOFCKUW5yoHnLAEAUPmU6XOWmjdvbn8w5B/NmTNHt9xyy7XsEgAA4Lp0TXOWnn/+eXXq1Elr1qyxT4ROSkpSSkpKoe9eAwAAqMyu6cpS27Zt9cMPP+jBBx9URkaGMjIy1L17d+3du1f/+9//SrtGAACACnNNc5auZOfOnbrtttuUl5dXWrusFJizBABA5VOmc5YAAAD+KghLAAAAFghLAAAAFkr0abju3btbrs/IyPgztQAAAFx3ShSW/Pz8rrq+X79+f6ogAACA60mJwtKCBQvKqg4AAIDrEnOWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALBCWAAAALFSasHT69GnFx8fL19dX/v7+GjRokM6ePWu5TXZ2toYNG6aAgAD5+PioR48eSktLs6/fuXOn4uLiFBoaKk9PTzVp0kQvvfRSWQ8FAABUIpUmLMXHx2vv3r1KSEjQJ598om+++UaDBw+23OaJJ57Qxx9/rOXLl+vrr7/W8ePH1b17d/v6bdu2qWbNmnrnnXe0d+9eTZo0SRMnTtScOXPKejgAAKCSsBljTEUXcTX79+9X06ZNtWXLFrVs2VKS9MUXX+j+++/XsWPHFBISUmibzMxM1ahRQ4sXL1bPnj0lScnJyWrSpImSkpJ0xx13FHmsYcOGaf/+/Vq7dm2x68vKypKfn58yMzPl6+t7DSMEAADlrbjv35XiylJSUpL8/f3tQUmSYmJi5OTkpE2bNhW5zbZt25Sbm6uYmBh7W+PGjVWnTh0lJSVd8ViZmZmqVq2aZT05OTnKyspyWAAAwI2pUoSl1NRU1axZ06HNxcVF1apVU2pq6hW3cXNzk7+/v0N7YGDgFbfZuHGjli5detXbezNmzJCfn599CQ0NLf5gAABApVKhYWnChAmy2WyWS3JycrnUsmfPHnXt2lXTpk3TfffdZ9l34sSJyszMtC8pKSnlUiMAACh/LhV58LFjx2rAgAGWferXr6+goCClp6c7tF+6dEmnT59WUFBQkdsFBQXp4sWLysjIcLi6lJaWVmibffv2qX379ho8eLAmT5581brd3d3l7u5+1X4AAKDyq9CwVKNGDdWoUeOq/aKjo5WRkaFt27apRYsWkqS1a9cqPz9frVq1KnKbFi1ayNXVVYmJierRo4ck6cCBAzp69Kiio6Pt/fbu3at7771X/fv31zPPPFMKowIAADeSSvFpOEnq2LGj0tLSNG/ePOXm5mrgwIFq2bKlFi9eLEn65Zdf1L59ey1atEhRUVGSpH/84x/67LPPtHDhQvn6+mrEiBGSfp+bJP1+6+3ee+9VbGysZs6caT+Ws7NzsUJcAT4NBwBA5VPc9+8KvbJUEu+++66GDx+u9u3by8nJST169NDLL79sX5+bm6sDBw7o/Pnz9rYXX3zR3jcnJ0exsbF67bXX7OtXrFihX3/9Ve+8847eeecde3vdunV1+PDhchkXAAC4vlWaK0vXM64sAQBQ+dxQz1kCAACoKIQlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC4QlAAAAC5UmLJ0+fVrx8fHy9fWVv7+/Bg0apLNnz1puk52drWHDhikgIEA+Pj7q0aOH0tLSiux76tQp1a5dWzabTRkZGWUwAgAAUBlVmrAUHx+vvXv3KiEhQZ988om++eYbDR482HKbJ554Qh9//LGWL1+ur7/+WsePH1f37t2L7Dto0CDdcsstZVE6AACoxGzGGFPRRVzN/v371bRpU23ZskUtW7aUJH3xxRe6//77dezYMYWEhBTaJjMzUzVq1NDixYvVs2dPSVJycrKaNGmipKQk3XHHHfa+c+fO1dKlSzV16lS1b99ev/32m/z9/YtdX1ZWlvz8/JSZmSlfX98/N1gAAFAuivv+XSmuLCUlJcnf398elCQpJiZGTk5O2rRpU5HbbNu2Tbm5uYqJibG3NW7cWHXq1FFSUpK9bd++fXr66ae1aNEiOTkV73Tk5OQoKyvLYQEAADemShGWUlNTVbNmTYc2FxcXVatWTampqVfcxs3NrdAVosDAQPs2OTk5iouL08yZM1WnTp1i1zNjxgz5+fnZl9DQ0JINCAAAVBoVGpYmTJggm81muSQnJ5fZ8SdOnKgmTZrokUceKfF2mZmZ9iUlJaWMKgQAABXNpSIPPnbsWA0YMMCyT/369RUUFKT09HSH9kuXLun06dMKCgoqcrugoCBdvHhRGRkZDleX0tLS7NusXbtWu3fv1ooVKyRJBdO3qlevrkmTJumpp54qct/u7u5yd3cvzhABAEAlV6FhqUaNGqpRo8ZV+0VHRysjI0Pbtm1TixYtJP0edPLz89WqVasit2nRooVcXV2VmJioHj16SJIOHDigo0ePKjo6WpL0/vvv68KFC/ZttmzZor///e9av369GjRo8GeHBwAAbgAVGpaKq0mTJurQoYMee+wxzZs3T7m5uRo+fLj69Olj/yTcL7/8ovbt22vRokWKioqSn5+fBg0apDFjxqhatWry9fXViBEjFB0dbf8k3OWB6OTJk/bjleTTcAAA4MZVKcKSJL377rsaPny42rdvLycnJ/Xo0UMvv/yyfX1ubq4OHDig8+fP29tefPFFe9+cnBzFxsbqtddeq4jyAQBAJVUpnrN0veM5SwAAVD431HOWAAAAKgphCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwAJhCQAAwIJLRRdwIzDGSJKysrIquBIAAFBcBe/bBe/jV0JYKgVnzpyRJIWGhlZwJQAAoKTOnDkjPz+/K663mavFKVxVfn6+jh8/ripVqshms1V0ORUqKytLoaGhSklJka+vb0WXc8PiPJcfznX54DyXD86zI2OMzpw5o5CQEDk5XXlmEleWSoGTk5Nq165d0WVcV3x9ffmHWA44z+WHc10+OM/lg/P8/1hdUSrABG8AAAALhCUAAAALhCWUKnd3d02bNk3u7u4VXcoNjfNcfjjX5YPzXD44z9eGCd4AAAAWuLIEAABggbAEAABggbAEAABggbAEAABggbCEEjt9+rTi4+Pl6+srf39/DRo0SGfPnrXcJjs7W8OGDVNAQIB8fHzUo0cPpaWlFdn31KlTql27tmw2mzIyMspgBJVDWZznnTt3Ki4uTqGhofL09FSTJk300ksvlfVQriuvvvqqwsLC5OHhoVatWmnz5s2W/ZcvX67GjRvLw8NDzZo102effeaw3hijqVOnKjg4WJ6enoqJidGPP/5YlkOoFErzPOfm5mr8+PFq1qyZvL29FRISon79+un48eNlPYzrXmm/nv9oyJAhstlsmj17dilXXQkZoIQ6dOhgmjdvbr777juzfv16c9NNN5m4uDjLbYYMGWJCQ0NNYmKi2bp1q7njjjtM69ati+zbtWtX07FjRyPJ/Pbbb2UwgsqhLM7zm2++aUaOHGm++uorc/DgQfO///3PeHp6mldeeaWsh3NdWLJkiXFzczNvvfWW2bt3r3nssceMv7+/SUtLK7L/hg0bjLOzs3n++efNvn37zOTJk42rq6vZvXu3vc+zzz5r/Pz8zKpVq8zOnTtNly5dTL169cyFCxfKa1jXndI+zxkZGSYmJsYsXbrUJCcnm6SkJBMVFWVatGhRnsO67pTF67nABx98YJo3b25CQkLMiy++WMYjuf4RllAi+/btM5LMli1b7G2ff/65sdls5pdffilym4yMDOPq6mqWL19ub9u/f7+RZJKSkhz6vvbaa6Zt27YmMTHxLx2Wyvo8/9HQoUPNPffcU3rFX8eioqLMsGHD7D/n5eWZkJAQM2PGjCL79+rVy3Tq1MmhrVWrVubxxx83xhiTn59vgoKCzMyZM+3rMzIyjLu7u3nvvffKYASVQ2mf56Js3rzZSDJHjhwpnaIrobI6z8eOHTO1atUye/bsMXXr1iUsGWO4DYcSSUpKkr+/v1q2bGlvi4mJkZOTkzZt2lTkNtu2bVNubq5iYmLsbY0bN1adOnWUlJRkb9u3b5+efvppLVq0yPILDf8KyvI8Xy4zM1PVqlUrveKvUxcvXtS2bdsczo+Tk5NiYmKueH6SkpIc+ktSbGysvf+hQ4eUmprq0MfPz0+tWrWyPOc3srI4z0XJzMyUzWaTv79/qdRd2ZTVec7Pz1ffvn01btw4RURElE3xldBf+x0JJZaamqqaNWs6tLm4uKhatWpKTU294jZubm6F/qMWGBho3yYnJ0dxcXGaOXOm6tSpUya1VyZldZ4vt3HjRi1dulSDBw8ulbqvZydPnlReXp4CAwMd2q3OT2pqqmX/gj9Lss8bXVmc58tlZ2dr/PjxiouL+8t+GWxZnefnnntOLi4uGjlyZOkXXYkRliBJmjBhgmw2m+WSnJxcZsefOHGimjRpokceeaTMjnE9qOjz/Ed79uxR165dNW3aNN13333lckzgz8rNzVWvXr1kjNHcuXMrupwbyrZt2/TSSy9p4cKFstlsFV3OdcWlogvA9WHs2LEaMGCAZZ/69esrKChI6enpDu2XLl3S6dOnFRQUVOR2QUFBunjxojIyMhyueqSlpdm3Wbt2rXbv3q0VK1ZI+v0TRpJUvXp1TZo0SU899dQ1juz6UtHnucC+ffvUvn17DR48WJMnT76msVQ21atXl7Ozc6FPYRZ1fgoEBQVZ9i/4My0tTcHBwQ59IiMjS7H6yqMsznOBgqB05MgRrV279i97VUkqm/O8fv16paenO1zdz8vL09ixYzV79mwdPny4dAdRmVT0pClULgUTj7du3WpvW716dbEmHq9YscLelpyc7DDx+KeffjK7d++2L2+99ZaRZDZu3HjFT3bcyMrqPBtjzJ49e0zNmjXNuHHjym4A16moqCgzfPhw+895eXmmVq1alhNiO3fu7NAWHR1daIL3Cy+8YF+fmZnJBO9SPs/GGHPx4kXTrVs3ExERYdLT08um8EqmtM/zyZMnHf47vHv3bhMSEmLGjx9vkpOTy24glQBhCSXWoUMHc+utt5pNmzaZb7/91oSHhzt8pP3YsWOmUaNGZtOmTfa2IUOGmDp16pi1a9earVu3mujoaBMdHX3FY6xbt+4v/Wk4Y8rmPO/evdvUqFHDPPLII+bEiRP25a/y5rNkyRLj7u5uFi5caPbt22cGDx5s/P39TWpqqjHGmL59+5oJEybY+2/YsMG4uLiYF154wezfv99MmzatyEcH+Pv7mw8//NDs2rXLdO3alUcHlPJ5vnjxounSpYupXbu22bFjh8NrNycnp0LGeD0oi9fz5fg03O8ISyixU6dOmbi4OOPj42N8fX3NwIEDzZkzZ+zrDx06ZCSZdevW2dsuXLhghg4daqpWrWq8vLzMgw8+aE6cOHHFYxCWyuY8T5s2zUgqtNStW7ccR1axXnnlFVOnTh3j5uZmoqKizHfffWdf17ZtW9O/f3+H/suWLTMNGzY0bm5uJiIiwnz66acO6/Pz882UKVNMYGCgcXd3N+3btzcHDhwoj6Fc10rzPBe81ota/vj6/ysq7dfz5QhLv7MZ8/9PDgEAAEAhfBoOAADAAmEJAADAAmEJAADAAmEJAADAAmEJAADAAmEJAADAAmEJAADAAmEJAMqAzWbTqlWrKroMAKWAsATghjNgwADZbLZCS4cOHSq6NACVkEtFFwAAZaFDhw5asGCBQ5u7u3sFVQOgMuPKEoAbkru7u4KCghyWqlWrSvr9FtncuXPVsWNHeXp6qn79+lqxYoXD9rt379a9994rT09PBQQEaPDgwTp79qxDn7feeksRERFyd3dXcHCwhg8f7rD+5MmTevDBB+Xl5aXw8HB99NFHZTtoAGWCsATgL2nKlCnq0aOHdu7cqfj4ePXp00f79++XJJ07d06xsbGqWrWqtmzZouXLl2vNmjUOYWju3LkaNmyYBg8erN27d+ujjz7STTfd5HCMp556Sr169dKuXbt0//33Kz4+XqdPny7XcQIoBRX9Tb4AUNr69+9vnJ2djbe3t8PyzDPPGGOMkWSGDBnisE2rVq3MP/7xD2OMMfPnzzdVq1Y1Z8+eta//9NNPjZOTk0lNTTXGGBMSEmImTZp0xRokmcmTJ9t/Pnv2rJFkPv/881IbJ4DywZwlADeke+65R3PnznVoq1atmv3v0dHRDuuio6O1Y8cOSdL+/fvVvHlzeXt729e3adNG+fn5OnDggGw2m44fP6727dtb1nDLLbfY/+7t7S1fX1+lp6df65AAVBDCEoAbkre3d6HbYqXF09OzWP1cXV0dfrbZbMrPzy+LkgCUIeYsAfhL+u677wr93KRJE0lSkyZNtHPnTp07d86+fsOGDXJyclKjRo1UpUoVhYWFKTExsVxrBlAxuLIE4IaUk5Oj1NRUhzYXFxdVr15dkrR8+XK1bNlSd955p959911t3rxZb775piQpPj5e06ZNU//+/TV9+nT9+uuvGjFihPr27avAwEBJ0vTp0zVkyBDVrFlTHTt21JkzZ7RhwwaNGDGifAcKoMwRlgDckL744gsFBwc7tDVq1EjJycmSfv+k2pIlSzR06FAFBwfrvffeU9OmTSVJXl5eWr16tUaNGqXbb79dXl5e6tGjh/773//a99W/f39lZ2frxRdf1JNPPqnq1aurZ8+e5TdAAOXGZowxFV0EAJQnm82mlStXqlu3bhVdCoBKgDlLAAAAFghLAAAAFpizBOAvh9kHAEqCK0sAAAAWCEsAAAAWCEsAAAAWCEsAAAAWCEsAAAAWCEsAAAAWCEsAAAAWCEsAAAAWCEsAAAAW/j/kbQDhLPuHsAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c6b824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 88/88 [05:57<00:00,  4.06s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_loader, desc=\"Validating\", leave=True):  # Wrap the DataLoader with tqdm\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)  # Raw logits\n",
    "        probabilities = torch.sigmoid(outputs).cpu().numpy()  # Convert logits to probabilities\n",
    "        y_pred.extend(probabilities)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d496f7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC for Infiltration: 0.6855\n",
      "AUROC for Atelectasis: 0.7382\n",
      "AUROC for Effusion: 0.7952\n",
      "AUROC for Nodule: 0.6755\n",
      "AUROC for Pneumothorax: 0.7887\n",
      "AUROC for No Finding: 0.7075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class_names = [\"Infiltration\", \"Atelectasis\", \"Effusion\", \"Nodule\", \"Pneumothorax\", \"No Finding\"]\n",
    "\n",
    "# Assuming y_true (ground truth) and y_pred (predictions) are numpy arrays\n",
    "# Shape: [num_samples, num_classes]\n",
    "aurocs = {}\n",
    "for i, class_name in enumerate(class_names):  # Replace class_names with your actual class names\n",
    "    try:\n",
    "        auroc = roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "        aurocs[class_name] = auroc\n",
    "    except ValueError:\n",
    "        # Handle case where a class has only one label value (all 0s or 1s)\n",
    "        aurocs[class_name] = None\n",
    "        print(f\"AUROC for {class_name} could not be calculated due to single-class labels.\")\n",
    "\n",
    "# Print AUROC for each class\n",
    "for class_name, auroc in aurocs.items():\n",
    "    if auroc is not None:\n",
    "        print(f\"AUROC for {class_name}: {auroc:.4f}\")\n",
    "    else:\n",
    "        print(f\"AUROC for {class_name}: Not Applicable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85daa02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average AUROC: 0.7318\n"
     ]
    }
   ],
   "source": [
    "valid_aurocs = [auroc for auroc in aurocs.values() if auroc is not None]\n",
    "average_auroc = sum(valid_aurocs) / len(valid_aurocs) if valid_aurocs else 0\n",
    "print(f\"Average AUROC: {average_auroc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d9a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[9]: Save Model\n",
    "torch.save(model.state_dict(), \"chest_xray_model.pth.tar\")\n",
    "print(\"Model saved as 'chest_xray_model.pth.tar'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
